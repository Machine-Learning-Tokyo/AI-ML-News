# pip install pytorch-transformers

PyTorch-Transformers (formely known as pytorch-pretrained-bert) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).

The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:

- **BERT (Google)** released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
- **GPT (OpenAI)** released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.
- **GPT-2 (OpenAI)** released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.
- **Transformer-XL (Google/CMU)** released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
- **XLNet (Google/CMU)** released with the paper â€‹XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
- **XLM (Facebook)** released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.

[<p align="center"><img src="https://github.com/Machine-Learning-Tokyo/AI-ML-Newsletter/blob/master/images/pytorchnlp.png" width="600"></p>](https://github.com/huggingface/pytorch-transformers/releases/tag/v1.0.0)

# ICLR 2020 Call for Papers

1st Call for Papers for #ICLR2020. See the full call online, https://iclr.cc/Conferences/2020/CallForPapers Submission deadline Wednesday, 25 September 2019, 6pm East Africa Time

# New fast.ai course: Natural Language Processing
Applications covered include topic modeling, classfication (identifying whether the sentiment of a review is postive or negative), language modeling, and translation. The course teaches a blend of traditional NLP topics (including regex, SVD, naive bayes, tokenization) and recent neural network approaches (including RNNs, seq2seq, attention, and the transformer architecture), as well as addressing urgent ethical issues, such as bias and disinformation. Topics can be watched in any order.
[<p align="center"><img src="https://github.com/Machine-Learning-Tokyo/AI-ML-Newsletter/blob/master/images/fastai1.png" width="600"></p>](https://www.fast.ai/2019/07/08/fastai-nlp/)



# Google ML Summit Tokyo

The Google ML Summit was held in July at Roppongi Hills with talks from Jeff Dean, Laurence Moroney, Paige Bailey, Heiga Zen, ...
[<p align="center"><img src="https://github.com/Machine-Learning-Tokyo/AI-ML-Newsletter/blob/master/images/googlemlsummit.png" width="600"></p>](https://events.withgoogle.com/mlsummit-tokyo-vol3/)
